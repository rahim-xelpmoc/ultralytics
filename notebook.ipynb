{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR,RTDETRWithEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RTDETR(\"D:\\\\Xelpmoc\\\\yolo_layout_model\\\\model\\\\best_rtdetr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\xelpmoc\\Desktop\\Screenshot_6-2-2025_122528_arxiv.org.jpeg: 640x640 13 Texts, 1 SectionHeader, 700.7ms\n",
      "Speed: 4.7ms preprocess, 700.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "out=model(\"C:\\\\Users\\\\xelpmoc\\\\Desktop\\\\Screenshot_6-2-2025_122528_arxiv.org.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].boxes.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\xelpmoc\\Desktop\\1LeaseAgreementColoradoSprings_6.jpg: 640x640 2 InlineHeaders, 5 Texts, 1 Picture, 711.3ms\n",
      "Speed: 5.0ms preprocess, 711.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "out2=model(\"C:\\\\Users\\\\xelpmoc\\\\Desktop\\\\1LeaseAgreementColoradoSprings_6.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([ 2.,  2.,  2.,  0.,  0.,  2., 12.,  2.])\n",
       "conf: tensor([0.9637, 0.9679, 0.9697, 0.8013, 0.7991, 0.4206, 0.3800, 0.4052])\n",
       "data: tensor([[3.1117e+02, 1.9224e+03, 2.1982e+03, 2.8357e+03, 9.6373e-01, 2.0000e+00],\n",
       "        [3.1596e+02, 1.0595e+03, 2.1991e+03, 1.8577e+03, 9.6793e-01, 2.0000e+00],\n",
       "        [3.2077e+02, 3.5548e+02, 2.1991e+03, 1.0051e+03, 9.6967e-01, 2.0000e+00],\n",
       "        [5.9375e+02, 1.0467e+03, 1.3032e+03, 1.1115e+03, 8.0130e-01, 0.0000e+00],\n",
       "        [5.9676e+02, 3.4618e+02, 1.5164e+03, 4.1268e+02, 7.9914e-01, 0.0000e+00],\n",
       "        [1.1566e+03, 2.9159e+03, 1.1861e+03, 2.9535e+03, 4.2055e-01, 2.0000e+00],\n",
       "        [1.8116e+03, 2.9259e+03, 2.1907e+03, 3.1372e+03, 3.7997e-01, 1.2000e+01],\n",
       "        [3.1096e+02, 2.8549e+03, 1.5664e+03, 2.8934e+03, 4.0525e-01, 2.0000e+00]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (3301, 2550)\n",
       "shape: torch.Size([8, 6])\n",
       "xywh: tensor([[1254.6877, 2379.0388, 1887.0289,  913.2352],\n",
       "        [1257.5127, 1458.5636, 1883.0963,  798.1804],\n",
       "        [1259.9293,  680.2737, 1878.3127,  649.5966],\n",
       "        [ 948.4631, 1079.0690,  709.4344,   64.7886],\n",
       "        [1056.5742,  379.4296,  919.6287,   66.5050],\n",
       "        [1171.3610, 2934.7358,   29.4817,   37.6189],\n",
       "        [2001.1359, 3031.5505,  379.1194,  211.3750],\n",
       "        [ 938.6680, 2874.1670, 1255.4141,   38.4539]])\n",
       "xywhn: tensor([[0.4920, 0.7207, 0.7400, 0.2767],\n",
       "        [0.4931, 0.4419, 0.7385, 0.2418],\n",
       "        [0.4941, 0.2061, 0.7366, 0.1968],\n",
       "        [0.3719, 0.3269, 0.2782, 0.0196],\n",
       "        [0.4143, 0.1149, 0.3606, 0.0201],\n",
       "        [0.4594, 0.8890, 0.0116, 0.0114],\n",
       "        [0.7848, 0.9184, 0.1487, 0.0640],\n",
       "        [0.3681, 0.8707, 0.4923, 0.0116]])\n",
       "xyxy: tensor([[ 311.1732, 1922.4213, 2198.2021, 2835.6565],\n",
       "        [ 315.9645, 1059.4734, 2199.0608, 1857.6538],\n",
       "        [ 320.7730,  355.4754, 2199.0857, 1005.0720],\n",
       "        [ 593.7458, 1046.6747, 1303.1803, 1111.4633],\n",
       "        [ 596.7599,  346.1771, 1516.3885,  412.6821],\n",
       "        [1156.6201, 2915.9265, 1186.1018, 2953.5454],\n",
       "        [1811.5762, 2925.8630, 2190.6956, 3137.2380],\n",
       "        [ 310.9610, 2854.9402, 1566.3750, 2893.3940]])\n",
       "xyxyn: tensor([[0.1220, 0.5824, 0.8620, 0.8590],\n",
       "        [0.1239, 0.3210, 0.8624, 0.5628],\n",
       "        [0.1258, 0.1077, 0.8624, 0.3045],\n",
       "        [0.2328, 0.3171, 0.5111, 0.3367],\n",
       "        [0.2340, 0.1049, 0.5947, 0.1250],\n",
       "        [0.4536, 0.8833, 0.4651, 0.8947],\n",
       "        [0.7104, 0.8864, 0.8591, 0.9504],\n",
       "        [0.1219, 0.8649, 0.6143, 0.8765]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[0].boxes.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Captured queries: torch.Size([1, 300, 256])\n",
      "Captured queries: torch.Size([1, 300, 256])\n",
      "image 1/1 C:\\Users\\xelpmoc\\Desktop\\1LeaseAgreementColoradoSprings_6.jpg: 640x640 2 InlineHeaders, 5 Texts, 1 Picture, 674.1ms\n",
      "Speed: 4.3ms preprocess, 674.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add a hook to capture decoder queries\n",
    "def hook_fn(module, input, output):\n",
    "    print(\"Captured queries:\", output.shape)  # Replace with your processing\n",
    "\n",
    "# Attach the hook to the desired layer\n",
    "decoder_layer = model.model.model[-1].decoder.layers[-1] # For example, the last decoder layer\n",
    "hook = decoder_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Perform a forward pass\n",
    "outputs = model(\"C:\\\\Users\\\\xelpmoc\\\\Desktop\\\\1LeaseAgreementColoradoSprings_6.jpg\")\n",
    "\n",
    "# Remove the hook after capturing\n",
    "hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeformableTransformerDecoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.0, inplace=True)\n",
       "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn): MSDeformAttn(\n",
       "    (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "    (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "    (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.0, inplace=True)\n",
       "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "  (act): ReLU(inplace=True)\n",
       "  (dropout3): Dropout(p=0.0, inplace=True)\n",
       "  (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (dropout4): Dropout(p=0.0, inplace=True)\n",
       "  (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model[-1].decoder.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=RTDETRWithEmbeddings(\"D:\\\\Xelpmoc\\\\yolo_layout_model\\\\model\\\\best_rtdetr.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\xelpmoc\\Desktop\\1LeaseAgreementColoradoSprings_6.jpg: 640x640 1 InlineHeader, 1 InlineHeader, 1 InlineHeader, 1 InlineHeader, 1 InlineHeader, 1 InlineHeader, 1 InlineHeader, 1 InlineHeader, 967.4ms\n",
      "Speed: 5.3ms preprocess, 967.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "out3=model2(\"C:\\\\Users\\\\xelpmoc\\\\Desktop\\\\1LeaseAgreementColoradoSprings_6.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3[0].boxes.data[0][5] # xyxy,conf, cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.metrics import box_iou\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path=\"D:\\\\Xelpmoc\\\\yolo_layout_model\\\\data\\\\project-4-at-2025-02-13-11-39-830bb9ae\\\\images\\\\0afb6316-GrandePrairieABLeaseAmendment_47.jpg\"\n",
    "xml_file_path=\"D:\\\\Xelpmoc\\\\yolo_layout_model\\\\data\\\\xml_layout\\\\xml_layout\\\\1LeaseAgreementDesPlaines_phase_5_0.xml\"\n",
    "with open(xml_file_path,'r') as file:\n",
    "    xml_markup=file.read()\n",
    "soup=BeautifulSoup(xml_markup,\"lxml-xml\")\n",
    "height=float(soup.img_height.text)\n",
    "width=float(soup.img_width.text)\n",
    "objects=soup.find_all('object')\n",
    "labels_with_polygon=[]\n",
    "bboxes=[]\n",
    "for object in objects:\n",
    "    polygon=object.polygon.text\n",
    "    labels_with_polygon.append((object.label.text,[float(coordinate) for coordinate in polygon.split(',')]))\n",
    "for label,polygon in labels_with_polygon:\n",
    "    xmin=polygon[0]\n",
    "    xmax=polygon[4]\n",
    "    ymin=polygon[1]\n",
    "    ymax=polygon[5]\n",
    "    bboxes.append([xmin,ymin,xmax,ymax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91.827392578125, 68.26171875, 346.97265625, 112.5], [93.487548828125, 198.6328125, 390.5517578125, 250.48828125], [112.164306640625, 266.455078125, 484.765625, 400.48828125], [114.447021484375, 413.671875, 475.2197265625, 516.796875], [129.4921875, 538.76953125, 355.6884765625, 597.36328125], [103.448486328125, 612.3046875, 361.083984375, 687.3046875], [80.621337890625, 700.78125, 322.4853515625, 785.15625], [72.735595703125, 798.6328125, 327.05078125, 874.8046875], [69.830322265625, 887.109375, 288.4521484375, 975.0], [76.470947265625, 979.6875, 352.3681640625, 1058.203125], [103.448486328125, 1061.1328125, 323.3154296875, 1153.7109375], [433.30078125, 561.03515625, 830.908203125, 583.30078125], [569.43359375, 643.9453125, 667.3828125, 672.0703125], [566.943359375, 695.5078125, 674.853515625, 717.7734375], [425.4150390625, 741.2109375, 834.6435546875, 762.3046875], [571.09375, 783.984375, 663.232421875, 805.078125], [611.3525390625, 895.8984375, 657.0068359375, 919.3359375], [396.3623046875, 984.375, 854.98046875, 1006.640625], [583.9599609375, 1051.171875, 676.9287109375, 1079.296875], [518.798828125, 1093.359375, 766.162109375, 1164.84375], [87.158203125, 1216.845703125, 1460.107421875, 1318.2861328125], [-28.6376953125, 1323.22998046875, 1284.9609375, 1487.841796875], [41.0888671875, 1497.36328125, 1442.67578125, 1630.17578125], [92.138671875, 1631.640625, 1521.533203125, 1752.24609375], [119.53125, 1752.24609375, 1527.34375, 1853.80859375], [115.7958984375, 1865.0390625, 1470.8984375, 1908.984375], [85.0830078125, 1926.5625, 1381.25, 1991.9921875], [128.2470703125, 2009.5703125, 1248.4375, 2042.7734375], [93.3837890625, 2054.98046875, 1472.55859375, 2109.66796875]]\n"
     ]
    }
   ],
   "source": [
    "print(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\Xelpmoc\\yolo_layout_model\\data\\project-4-at-2025-02-13-11-39-830bb9ae\\images\\0afb6316-GrandePrairieABLeaseAmendment_47.jpg: 640x640 6 Texts, 5 SectionHeaders, 690.9ms\n",
      "Speed: 6.3ms preprocess, 690.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "out=model(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].boxes.xyxy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1299, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0233, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000],\n",
       "        [0.0370, 0.0000, 0.0156, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0327],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1681, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1466, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_iou(torch.tensor(bboxes),out[0].boxes.xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_boxes=out[0].boxes.xyxy.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw,Image\n",
    "def draw_yolo_bboxes(image_path, yolo_txt_path,box2):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size  # Image dimensions\n",
    "\n",
    "    # Read YOLO annotation file\n",
    "    with open(yolo_txt_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Create drawing object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Process each bounding box\n",
    "    boxes=[]\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id, x_center, y_center, w, h = map(float, parts)\n",
    "\n",
    "        # Convert from YOLO format to pixel values\n",
    "        x_center *= width\n",
    "        y_center *= height\n",
    "        w *= width\n",
    "        h *= height\n",
    "\n",
    "        # Get top-left and bottom-right coordinates\n",
    "        x1 = int(x_center - w / 2)\n",
    "        y1 = int(y_center - h / 2)\n",
    "        x2 = int(x_center + w / 2)\n",
    "        y2 = int(y_center + h / 2)\n",
    "\n",
    "        # Draw rectangle\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "    for box in box2:\n",
    "        x1,y1,x2,y2 =box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"blue\", width=2)\n",
    "    \n",
    "    image.show()\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes=draw_yolo_bboxes(image_path,\"D:\\\\Xelpmoc\\\\yolo_layout_model\\\\data\\\\project-4-at-2025-02-13-11-39-830bb9ae\\\\labels\\\\0afb6316-GrandePrairieABLeaseAmendment_47.txt\",predicted_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_scores=box_iou(torch.tensor(boxes),out[0].boxes.xyxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 11])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9148, 0.9681, 0.8892, 0.9213, 0.9219, 0.9747, 0.9594, 0.9438, 0.6922]),\n",
       "indices=tensor([4, 2, 5, 3, 9, 0, 6, 1, 7]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_scores.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
